{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a9c4fa",
   "metadata": {
    "cellId": "n5adcsenc7s7fleikt06b"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os,time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as l\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5535d",
   "metadata": {
    "cellId": "zhtpvgyibrrwf7m0uon79n",
    "execution_id": "6682699e-ce62-4a99-8118-ec741bd84fb4"
   },
   "source": [
    "<h1>DenseNets Tiramisu.</h1>\n",
    "<h3>Отныне и впредь я воссоздаю эту архитектуру модели. Выбор пал на неё из-за её превосходства над другими моделями сегментации. Данная модель способна отлично сегментировать разные данные. Модель будет перенесена и адаптирована мной для улучшения выполнения задачи. Спасибо за внимание!</h3>\n",
    "<h6>Код для переработки был взят из <a href='https://github.com/ShashiAI/FC-DenseNet-Tiramisu'>этого источника</a></h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8bbc6",
   "metadata": {
    "cellId": "ir7vrottxccjqkjqijzfv"
   },
   "outputs": [],
   "source": [
    "def preact_conv(inputs, n_filters, filter_size=[3, 3], dropout_p=0.2):\n",
    "    \"\"\"\n",
    "    Стандартный предактивационный слой для DenseNets-а, \n",
    "    Добавляет пакетную нормализацию, нелинейную ReLU, свёртку и выпадение. \n",
    "    \"\"\"\n",
    "    \n",
    "    norm = l.BatchNormalization(inputs=inputs)\n",
    "    preact = l.relu()(norm)\n",
    "    conv = l.Conv2D(n_filters, filter_size)(preact)\n",
    "    if dropout_p != 0.0:\n",
    "        conv = l.Dropout(1.0-dropout_p)(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def DenseBLock(stack, n_layers, growth_rate, dropout_p, scope=None):\n",
    "    \"\"\"\n",
    "    Полносвязный блок для нейросети.\n",
    "    :param stack входной четырёхмерный тензор\n",
    "    :param n_layers число внутренних слоёв\n",
    "    :param growth_rate число карт признаков на внутренний слой\n",
    "    :return stack четырёхмерный тензор новых карт признаков\n",
    "    :return new_features четырёхмерный тензор, содержащий ТОЛЬКО новые карты признаков из этого блока\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(scope) as sc:\n",
    "        new_features = []\n",
    "        for i in range(n_layers):\n",
    "            # Подсчёт новых карт признаков\n",
    "            layer = preact_conv(stack, growth_rate, dropout_p=dropout_p)\n",
    "            new_features.append(layer)\n",
    "            # Укладывание нового слоя\n",
    "            stack = tf.concat([stack, layer], axis=-1)\n",
    "        new_features = tf.concat(new_features, axis=-1)\n",
    "        return stack, new_features\n",
    "    \n",
    "\n",
    "def TransitionLayer(inputs, n_filters, dropout_p=0.2, compression=1.0, scope=None):\n",
    "    \"\"\"\n",
    "    Переходный слой\n",
    "    Добавляет преактивационный блок и 2х2 пуллинг\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(scope) as sc:\n",
    "        if compression < 1.0:\n",
    "            n_filters = tf.to_int32(tf.floor(n_filters*compression))\n",
    "        l = preact_conv(inputs, n_filters, filter_size=[1, 1], dropout_p=dropour_p)\n",
    "        l = l.AveragePooling2D((2, 2), strides=(2, 2))(l)\n",
    "        return l\n",
    "    \n",
    "\n",
    "def TransitionDown(inputs, n_filters, dropout_p=0.2, scope=None):\n",
    "    \"\"\"\n",
    "    Слой перехода вниз\n",
    "    Преактивационный слой + 2х2 максимальный пуллинг\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(scope) as sc:\n",
    "        l = preact_conv(inputs, n_filters, filter_size=[1, 1], dropout_p=dropout_p)\n",
    "        l = l.MaxPooling2D([2, 2])(l)\n",
    "        return l\n",
    "    \n",
    "\n",
    "def TransitionUp(block_to_upsample, skip_connectiion, n_filters_keep, scope=None):\n",
    "    \"\"\"\n",
    "    Слой перехода вверх\n",
    "    Производит повышающую дискретизацию на block_to_unsample с фактором 2 и складывает с skip_connetion\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(scope) as sc:\n",
    "        # ПОвышаюющая дискретизация\n",
    "        l = l.Conv2DTranspose(n_filters_keep, kernel_size=[3, 3], stride=[2, 2])(block_to_upsample)\n",
    "        # Сложение с skip_connection\n",
    "        l = tf.concat([l, skip_connection], axis=-1)\n",
    "        return l\n",
    "    \n",
    "\n",
    "def build_fc_densenet(inputs, preset_model='FC-DenseNet56', num_classes=12, n_filters_first_conv=48, n_pool=5, growth_rate=12, n_layers_per_block=4, dropout_p=.2, scope=None):\n",
    "    \"\"\"\n",
    "    Создание итоговой модели\n",
    "    :param str preset_model пресетная модель (FC-DenseNet56, FC-DenseNet67, FC-DenseNet103)\n",
    "    :param int n_classes число классов\n",
    "    :param int n_filters_first_conv число фильтров для первой свёртки\n",
    "    :param int n_pool число слоёв пуллинга = слоёв перехода вниз/вверх\n",
    "    :param int growth_rate число создаваемых новых карт признаков в слое за блок\n",
    "    :param n_layers_per_block число слоёв на блок. Может быть числом или списком размером 2 * n_pool+1\n",
    "    :param float dropout_p коэффициент выпадения для каждой свёртки (0 если не нужен)\n",
    "    \"\"\"\n",
    "    \n",
    "    if preset_model == 'FC-DenseNet56':\n",
    "        n_pool=5\n",
    "        growth_rate=12\n",
    "        n_layers_per_block=4\n",
    "    elif preset_model == 'FC-DenseNet67':\n",
    "        n_pool=5\n",
    "        growth_rate=16\n",
    "        n_layers_per_block=5\n",
    "    elif preset_model == 'FC-DenseNet103':\n",
    "        n_pool=5\n",
    "        growth_rate=16\n",
    "        n_layers_per_block=[4, 5, 7, 10, 12, 15, 12, 10, 7, 5, 4]\n",
    "        \n",
    "    if type(n_layers_per_block) == list:\n",
    "        assert (len(n_layers_per_block) == 2 * n_pool + 1)\n",
    "    elif type(n_layers_per_block) == int:\n",
    "        n_layers_per_block = [n_layers_per_block] * (2 * n_pool + 1)\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    with tf.variable_scope(scope, preset_model, [inputs]) as sc:\n",
    "        \n",
    "        # Первая свёртка #\n",
    "        stack = l.Conv2D(inputs, n_filters_first_conv, [3, 3], name='first_conv')\n",
    "        n_filters = n_filters_first_conv\n",
    "        \n",
    "        # Понижаем дискретизацию #\n",
    "        skip_connection_list = []\n",
    "        \n",
    "        for i in range(n_pool):\n",
    "            # Полносвязный блок\n",
    "            stack, _ = DenseBlock(stack, n_layers_per_block[i], growth_rate, dropout_p, name='denseblock%d'%(i+1))\n",
    "            n_filters += growth_rate * n_layers_per_block[i]\n",
    "            # Под конец полносвязного блока текущий стек сохраняется в skip_connetions_list\n",
    "            skip_connection_list.append(stack)\n",
    "            \n",
    "            # Переход ниже\n",
    "            stack = TransitionDown(stack, n_filters, dropout_p, name='transitiondown%d'%(i+1))\n",
    "        \n",
    "        skip_connection_list = skip_connection_list[::-1]\n",
    "        \n",
    "        # Дно нейросети #\n",
    "        \n",
    "        # Мы будем поднимать только новые карты признаков\n",
    "        stack, block_to_upsample = DenseBlock(stack, n_layers_per_block[n_pool], growth_rate, dropout_p, name='denseblockbn%d'%(n_pool+1))\n",
    "        \n",
    "        # Повышаем дискретизацию #\n",
    "        \n",
    "        for i in range(n_pool):\n",
    "            # Переход выше (повышение дискр. + сложение с skip_connection)\n",
    "            n_filters_keep = growth_rate * n_layers_per_block[n_pool + 1]\n",
    "            stack = TransitionUp(block_to_upsample, skip_connection_list[i], n_filters_keep, name='transitionup%d' % (n_pool + i + 1))\n",
    "            \n",
    "            # Мы будем поднимать только новые карты признаков\n",
    "            stack, block_to_upsample = DenseBlock(stack, n_layers_per_block[n_pool + i + 1], growth_rate, dropout_p, name='denseblock%d' % (n_pool + i + 2))\n",
    "        \n",
    "        # Финальный Softmax #\n",
    "        \n",
    "        net = l.Conv2D(stack, num_classes, [1, 1], name='logits')\n",
    "        return net\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "836d1f02-b2bb-4497-a0cf-9ff0693dc4cc",
  "notebookPath": "Background_deleter/FC_DenseNet_Tiramisu.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
